{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akx9ZTKH3EZs"
   },
   "source": [
    "# Overfitting & Regularization\n",
    "\n",
    "- IMBD 를 이용하여 overfitting 과 regularization test  \n",
    "- Colab에서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1lY9Upj3EZu"
   },
   "source": [
    "num_words=NUM_WORDS는 데이터셋에서 빈도가 가장 높은 NUM_WORDS(여기서는 10,000)개의 단어만 사용하겠다는 의미입니다. 이 방식을 사용하는 이유는 메모리를 효율적으로 사용하고, 매우 드물게 등장하는 단어는 분석에 큰 영향을 미치지 않는다는 가정 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryTjY2W73EZv"
   },
   "source": [
    "keras.datasets.imdb 데이터셋은 이미 전처리되어 있어 각 단어가 숫자로 인코딩되어 있습니다. 원래의 단어로 복원하려면 해당 숫자에 해당하는 단어를 찾아야 합니다. keras.datasets.imdb는 단어와 해당 인덱스를 매핑한 딕셔너리를 제공합니다.\n",
    "\n",
    "다음은 이 딕셔너리를 사용하여 인코딩된 리뷰를 원래의 텍스트로 변환하는 방법입니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqy1VVR83EZw"
   },
   "source": [
    "다음의 요령으로 train_data, test_data 를 **multi_hot_encoding** 한다.  \n",
    "\n",
    "다중핫인코딩(multi-hot-encoding)은 문서를 sparse 이진 vector로 표시. 다중 핫 인코딩은 데이터의 존재 여부를 이진 값으로 표현하고, 구성 및 순서 정보를 포함하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수는 주어진 시퀀스(단어의 인덱스로 이루어진 리스트)를 multi-hot 인코딩하여 변환합니다. 각 시퀀스는 dim차원의 벡터로 변환되며, 이 벡터에서 시퀀스에 속하는 각 단어의 위치에 해당하는 요소의 값이 1.0이 됩니다. 이렇게 생성된 벡터들을 담은 numpy 배열을 반환합니다. 이 함수는 주어진 텍스트 데이터를 딥러닝 모델에 입력할 수 있는 형태로 변환하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLZz0D9k3EZx"
   },
   "source": [
    "## 3 가지 크기의 model 을 만들어 underfitting, overfitting 을 비교\n",
    "\n",
    "- 언어 모델은 overfitting 이 발생하기 쉬우므로 언어 모델을 이용하여 테스트\n",
    "\n",
    "이 모델은 입력 데이터를 받아 두 개의 은닉층을 거쳐 최종적으로 이진 분류를 수행하는 모델입니다. 가중치 정규화와 드롭아웃을 통해 과적합을 방지하고, 'relu'와 'sigmoid'를 활성화 함수로 사용하여 비선형성을 추가합니다.\n",
    "\n",
    "- metrics = ['accuracy', 'binary_crossentropy'] 로 주고 history.histroy['val_binary_crossentropy'] 를 서로 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No regularization, no dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FURZqd_L3EZx"
   },
   "source": [
    "### 한 figure 에 model_1,2,3 를 모두 그리고, 각각의 train / validation loss 를 같은 색으로 그린다.\n",
    "\n",
    "- history.epoch : epoch 수  \n",
    "\n",
    "\n",
    "- history.history['binary_crossentropy'] : train loss  \n",
    "\n",
    "\n",
    "- history.history['val_binary_crossentropy'] : validation loss  \n",
    "\n",
    "\n",
    "- plt.plot return value 의 get_color() 를 이용해 같은 색으로 그릴 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_yUwgHA3EZx"
   },
   "source": [
    "## l2 regularization 을 적용하여 같은 Test\n",
    "\n",
    "keras.regularizers.l2(0.001)는 L2 정규화 객체의 인스턴스를 생성합니다. 여기서 0.001은 정규화 강도를 나타냅니다. L2 정규화는 가중치 감쇠(weight decay)라고도 불리며, 신경망에서 과적합을 방지하기 위해 손실 함수에 패널티 항을 추가하는 일반적으로 사용되는 기법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KI6h4Hm3EZy"
   },
   "source": [
    "## Dropout 을 적용하여 같은 Test"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
